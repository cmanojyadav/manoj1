[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Manoj Yadav Chinthalaboina",
    "section": "",
    "text": "I am Manoj from India currently in my third semester in Advanced Data Analytics in University of North Texas. I have completed Computer Science in my undergraduate studies followed by my MBA degree. Later I worked for four years in software services industry as a business developer. I have a fond interest in Mathematics from childhood which lead my way to pursue analytics."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "I am delighted to welcome you to our blog, where the essence of collaboration and achievement converge. In this space, we proudly present a project performed and meticulously documented in our project portfolio. As you journey through the intricacies of our endeavors, I am confident you’ll find inspiration, insights, and perhaps even a spark of innovation. I am thrilled to have you here, and I invite you to immerse yourself in the narrative of our collective dedication and success. Let this blog post be a testament to the incredible possibilities that unfold when passion meets purpose.\n\nEnjoy the exploration!"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "R Main concepts blog post",
    "section": "",
    "text": "In this R script, we explore and analyze the mtcars dataset using the ggplot2 library and statistical functions. After loading the dataset, we visualize the relationship between miles per gallon and horsepower with a scatter plot. Subsequently, we subset the data to focus on cars with over 100 horsepower and perform a t-test to compare miles per gallon between high and low horsepower cars. A linear regression model is then constructed to predict miles per gallon based on horsepower, and the model is summarized for insights. Lastly, we demonstrate the use of the linear model to predict the miles per gallon for a new car with 150 horsepower, showcasing a comprehensive approach to data exploration, visualization, hypothesis testing, and predictive modeling in R.\n\n# Load required libraries\nlibrary(ggplot2)\nlibrary(ggthemes)\n\n# Load the built-in mtcars dataset\ndata(mtcars)\n\n# Display the structure of the dataset\nstr(mtcars)\n\n'data.frame':   32 obs. of  11 variables:\n $ mpg : num  21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ...\n $ cyl : num  6 6 4 6 8 6 8 4 4 6 ...\n $ disp: num  160 160 108 258 360 ...\n $ hp  : num  110 110 93 110 175 105 245 62 95 123 ...\n $ drat: num  3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ...\n $ wt  : num  2.62 2.88 2.32 3.21 3.44 ...\n $ qsec: num  16.5 17 18.6 19.4 17 ...\n $ vs  : num  0 0 1 1 0 1 0 1 1 1 ...\n $ am  : num  1 1 1 0 0 0 0 0 0 0 ...\n $ gear: num  4 4 4 3 3 3 3 4 4 4 ...\n $ carb: num  4 4 1 1 2 1 4 2 2 4 ...\n\n# Display the first few rows of the dataset\nhead(mtcars)\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\n# Summary statistics of the dataset\nsummary(mtcars)\n\n      mpg             cyl             disp             hp       \n Min.   :10.40   Min.   :4.000   Min.   : 71.1   Min.   : 52.0  \n 1st Qu.:15.43   1st Qu.:4.000   1st Qu.:120.8   1st Qu.: 96.5  \n Median :19.20   Median :6.000   Median :196.3   Median :123.0  \n Mean   :20.09   Mean   :6.188   Mean   :230.7   Mean   :146.7  \n 3rd Qu.:22.80   3rd Qu.:8.000   3rd Qu.:326.0   3rd Qu.:180.0  \n Max.   :33.90   Max.   :8.000   Max.   :472.0   Max.   :335.0  \n      drat             wt             qsec             vs        \n Min.   :2.760   Min.   :1.513   Min.   :14.50   Min.   :0.0000  \n 1st Qu.:3.080   1st Qu.:2.581   1st Qu.:16.89   1st Qu.:0.0000  \n Median :3.695   Median :3.325   Median :17.71   Median :0.0000  \n Mean   :3.597   Mean   :3.217   Mean   :17.85   Mean   :0.4375  \n 3rd Qu.:3.920   3rd Qu.:3.610   3rd Qu.:18.90   3rd Qu.:1.0000  \n Max.   :4.930   Max.   :5.424   Max.   :22.90   Max.   :1.0000  \n       am              gear            carb      \n Min.   :0.0000   Min.   :3.000   Min.   :1.000  \n 1st Qu.:0.0000   1st Qu.:3.000   1st Qu.:2.000  \n Median :0.0000   Median :4.000   Median :2.000  \n Mean   :0.4062   Mean   :3.688   Mean   :2.812  \n 3rd Qu.:1.0000   3rd Qu.:4.000   3rd Qu.:4.000  \n Max.   :1.0000   Max.   :5.000   Max.   :8.000  \n\n# Plotting a scatter plot of mpg against hp with ggplot2\nggplot(mtcars, aes(x = hp, y = mpg)) +\n  geom_point(color = \"skyblue\") +  # Add color to points\n  labs(title = \"Scatter Plot of MPG against Horsepower\",\n       x = \"Horsepower\",\n       y = \"Miles per Gallon\") +\n  theme_minimal()  # Use a minimal theme\n\n\n\n# Create a subset of the dataset for cars with more than 100 horsepower\nhigh_hp_cars &lt;- subset(mtcars, hp &gt; 100)\n\n# Display the first few rows of the subset\nhead(high_hp_cars)\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\nDuster 360        14.3   8  360 245 3.21 3.570 15.84  0  0    3    4\n\n# Perform a t-test comparing miles per gallon (mpg) between high and low horsepower cars\nt_test_result &lt;- t.test(mtcars$mpg, high_hp_cars$mpg)\nprint(t_test_result)\n\n\n    Welch Two Sample t-test\n\ndata:  mtcars$mpg and high_hp_cars$mpg\nt = 1.906, df = 52.988, p-value = 0.06209\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.1381627  5.4150648\nsample estimates:\nmean of x mean of y \n 20.09062  17.45217 \n\n# Linear regression model to predict mpg based on horsepower\nlinear_model &lt;- lm(mpg ~ hp, data = mtcars)\n\n# Display the summary of the linear model\nsummary(linear_model)\n\n\nCall:\nlm(formula = mpg ~ hp, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.7121 -2.1122 -0.8854  1.5819  8.2360 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 30.09886    1.63392  18.421  &lt; 2e-16 ***\nhp          -0.06823    0.01012  -6.742 1.79e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.863 on 30 degrees of freedom\nMultiple R-squared:  0.6024,    Adjusted R-squared:  0.5892 \nF-statistic: 45.46 on 1 and 30 DF,  p-value: 1.788e-07\n\n# Predict the mpg for a new car with 150 horsepower\nnew_car_hp &lt;- 150\npredicted_mpg &lt;- predict(linear_model, newdata = data.frame(hp = new_car_hp))\nprint(paste(\"Predicted MPG for a car with\", new_car_hp, \"horsepower:\", round(predicted_mpg, 2)))\n\n[1] \"Predicted MPG for a car with 150 horsepower: 19.86\"\n\n# Visualize the linear regression line along with the data points\nggplot(mtcars, aes(x = hp, y = mpg)) +\n  geom_point(color = \"skyblue\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +  # Add regression line\n  labs(title = \"Linear Regression: MPG against Horsepower\",\n       x = \"Horsepower\",\n       y = \"Miles per Gallon\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n# Residual plot to check model assumptions\nresiduals &lt;- resid(linear_model)\nggplot(mtcars, aes(x = hp, y = residuals)) +\n  geom_point(color = \"orange\") +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"blue\") +  # Add horizontal line at y = 0\n  labs(title = \"Residual Plot\",\n       x = \"Horsepower\",\n       y = \"Residuals\") +\n  theme_minimal()\n\n\n\n\nThe linear regression analysis of the mtcars dataset reveals a significant negative relationship between horsepower and miles per gallon (mpg). The model indicates that, on average, each unit increase in horsepower is associated with a decrease of 0.0682 units in mpg. The overall fit of the model is reasonable, with a low p-value and an adjusted R-squared value of 0.5892, indicating that approximately 58.92% of the variability in mpg can be explained by the linear relationship with horsepower. In practical terms, higher horsepower tends to be linked with lower fuel efficiency in the examined dataset."
  },
  {
    "objectID": "index.html#info",
    "href": "index.html#info",
    "title": "Manoj Yadav Chinthalaboina",
    "section": "",
    "text": "I am Manoj from India currently in my third semester in Advanced Data Analytics in University of North Texas. I have completed Computer Science in my undergraduate studies followed by my MBA degree. Later I worked for four years in software services industry as a business developer. I have a fond interest in Mathematics from childhood which lead my way to pursue analytics."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "blog post",
    "section": "",
    "text": "R Main explorationsS\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\nEDA\n\n\nMachine learining\n\n\n\n\n\n\n\nManoj Yadav Chinthalaboina\n\n\nDec 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR Main concepts blog post\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\nManoj Yadav Chinthalaboina\n\n\nDec 14, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\nnews\n\n\nupdates\n\n\n\n\n\n\n\nManoj Yadav Chinthalaboina\n\n\nDec 11, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/post-with-code/index.html#machine-learning-using-real-data-set",
    "href": "posts/post-with-code/index.html#machine-learning-using-real-data-set",
    "title": "R Main concepts blog post",
    "section": "",
    "text": "In this R script, we explore and analyze the mtcars dataset using the ggplot2 library and statistical functions. After loading the dataset, we visualize the relationship between miles per gallon and horsepower with a scatter plot. Subsequently, we subset the data to focus on cars with over 100 horsepower and perform a t-test to compare miles per gallon between high and low horsepower cars. A linear regression model is then constructed to predict miles per gallon based on horsepower, and the model is summarized for insights. Lastly, we demonstrate the use of the linear model to predict the miles per gallon for a new car with 150 horsepower, showcasing a comprehensive approach to data exploration, visualization, hypothesis testing, and predictive modeling in R.\n\n# Load required libraries\nlibrary(ggplot2)\nlibrary(ggthemes)\n\n# Load the built-in mtcars dataset\ndata(mtcars)\n\n# Display the structure of the dataset\nstr(mtcars)\n\n'data.frame':   32 obs. of  11 variables:\n $ mpg : num  21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ...\n $ cyl : num  6 6 4 6 8 6 8 4 4 6 ...\n $ disp: num  160 160 108 258 360 ...\n $ hp  : num  110 110 93 110 175 105 245 62 95 123 ...\n $ drat: num  3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ...\n $ wt  : num  2.62 2.88 2.32 3.21 3.44 ...\n $ qsec: num  16.5 17 18.6 19.4 17 ...\n $ vs  : num  0 0 1 1 0 1 0 1 1 1 ...\n $ am  : num  1 1 1 0 0 0 0 0 0 0 ...\n $ gear: num  4 4 4 3 3 3 3 4 4 4 ...\n $ carb: num  4 4 1 1 2 1 4 2 2 4 ...\n\n# Display the first few rows of the dataset\nhead(mtcars)\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\n# Summary statistics of the dataset\nsummary(mtcars)\n\n      mpg             cyl             disp             hp       \n Min.   :10.40   Min.   :4.000   Min.   : 71.1   Min.   : 52.0  \n 1st Qu.:15.43   1st Qu.:4.000   1st Qu.:120.8   1st Qu.: 96.5  \n Median :19.20   Median :6.000   Median :196.3   Median :123.0  \n Mean   :20.09   Mean   :6.188   Mean   :230.7   Mean   :146.7  \n 3rd Qu.:22.80   3rd Qu.:8.000   3rd Qu.:326.0   3rd Qu.:180.0  \n Max.   :33.90   Max.   :8.000   Max.   :472.0   Max.   :335.0  \n      drat             wt             qsec             vs        \n Min.   :2.760   Min.   :1.513   Min.   :14.50   Min.   :0.0000  \n 1st Qu.:3.080   1st Qu.:2.581   1st Qu.:16.89   1st Qu.:0.0000  \n Median :3.695   Median :3.325   Median :17.71   Median :0.0000  \n Mean   :3.597   Mean   :3.217   Mean   :17.85   Mean   :0.4375  \n 3rd Qu.:3.920   3rd Qu.:3.610   3rd Qu.:18.90   3rd Qu.:1.0000  \n Max.   :4.930   Max.   :5.424   Max.   :22.90   Max.   :1.0000  \n       am              gear            carb      \n Min.   :0.0000   Min.   :3.000   Min.   :1.000  \n 1st Qu.:0.0000   1st Qu.:3.000   1st Qu.:2.000  \n Median :0.0000   Median :4.000   Median :2.000  \n Mean   :0.4062   Mean   :3.688   Mean   :2.812  \n 3rd Qu.:1.0000   3rd Qu.:4.000   3rd Qu.:4.000  \n Max.   :1.0000   Max.   :5.000   Max.   :8.000  \n\n# Plotting a scatter plot of mpg against hp with ggplot2\nggplot(mtcars, aes(x = hp, y = mpg)) +\n  geom_point(color = \"skyblue\") +  # Add color to points\n  labs(title = \"Scatter Plot of MPG against Horsepower\",\n       x = \"Horsepower\",\n       y = \"Miles per Gallon\") +\n  theme_minimal()  # Use a minimal theme\n\n\n\n# Create a subset of the dataset for cars with more than 100 horsepower\nhigh_hp_cars &lt;- subset(mtcars, hp &gt; 100)\n\n# Display the first few rows of the subset\nhead(high_hp_cars)\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\nDuster 360        14.3   8  360 245 3.21 3.570 15.84  0  0    3    4\n\n# Perform a t-test comparing miles per gallon (mpg) between high and low horsepower cars\nt_test_result &lt;- t.test(mtcars$mpg, high_hp_cars$mpg)\nprint(t_test_result)\n\n\n    Welch Two Sample t-test\n\ndata:  mtcars$mpg and high_hp_cars$mpg\nt = 1.906, df = 52.988, p-value = 0.06209\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.1381627  5.4150648\nsample estimates:\nmean of x mean of y \n 20.09062  17.45217 \n\n# Linear regression model to predict mpg based on horsepower\nlinear_model &lt;- lm(mpg ~ hp, data = mtcars)\n\n# Display the summary of the linear model\nsummary(linear_model)\n\n\nCall:\nlm(formula = mpg ~ hp, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.7121 -2.1122 -0.8854  1.5819  8.2360 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 30.09886    1.63392  18.421  &lt; 2e-16 ***\nhp          -0.06823    0.01012  -6.742 1.79e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.863 on 30 degrees of freedom\nMultiple R-squared:  0.6024,    Adjusted R-squared:  0.5892 \nF-statistic: 45.46 on 1 and 30 DF,  p-value: 1.788e-07\n\n# Predict the mpg for a new car with 150 horsepower\nnew_car_hp &lt;- 150\npredicted_mpg &lt;- predict(linear_model, newdata = data.frame(hp = new_car_hp))\nprint(paste(\"Predicted MPG for a car with\", new_car_hp, \"horsepower:\", round(predicted_mpg, 2)))\n\n[1] \"Predicted MPG for a car with 150 horsepower: 19.86\"\n\n# Visualize the linear regression line along with the data points\nggplot(mtcars, aes(x = hp, y = mpg)) +\n  geom_point(color = \"skyblue\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +  # Add regression line\n  labs(title = \"Linear Regression: MPG against Horsepower\",\n       x = \"Horsepower\",\n       y = \"Miles per Gallon\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n# Residual plot to check model assumptions\nresiduals &lt;- resid(linear_model)\nggplot(mtcars, aes(x = hp, y = residuals)) +\n  geom_point(color = \"orange\") +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"blue\") +  # Add horizontal line at y = 0\n  labs(title = \"Residual Plot\",\n       x = \"Horsepower\",\n       y = \"Residuals\") +\n  theme_minimal()\n\n\n\n\nThe linear regression analysis of the mtcars dataset reveals a significant negative relationship between horsepower and miles per gallon (mpg). The model indicates that, on average, each unit increase in horsepower is associated with a decrease of 0.0682 units in mpg. The overall fit of the model is reasonable, with a low p-value and an adjusted R-squared value of 0.5892, indicating that approximately 58.92% of the variability in mpg can be explained by the linear relationship with horsepower. In practical terms, higher horsepower tends to be linked with lower fuel efficiency in the examined dataset."
  },
  {
    "objectID": "posts/post-with-code/index.html#svsm-random-forest-decsision-tree-and-naive-bayes-machine-learning-model",
    "href": "posts/post-with-code/index.html#svsm-random-forest-decsision-tree-and-naive-bayes-machine-learning-model",
    "title": "R Main concepts blog post",
    "section": "SVSM RANDOM FOREST DECSISION TREE AND NAIVE BAYES MACHINE LEARNING MODEL",
    "text": "SVSM RANDOM FOREST DECSISION TREE AND NAIVE BAYES MACHINE LEARNING MODEL\nIn this code, we are exploring the application of different machine learning models to a randomly generated dataset. The dataset consists of three features (Feature1, Feature2, and Feature3) and a binary target variable (Target) with classes “Low” and “High.” Four distinct models are trained on the training set: Support Vector Machine (SVM), Random Forest, Decision Tree, and Naive Bayes. Subsequently, each model is used to make predictions on the test set, and confusion matrices are generated to evaluate their performance. The confusion matrices provide insights into the accuracy of predictions, highlighting true positive, true negative, false positive, and false negative outcomes for each model. This analysis serves to compare and contrast the effectiveness of the diverse machine learning approaches employed in the context of the given dataset.\n\n# Load necessary libraries\nlibrary(ggplot2)\nlibrary(caret)\n\nLoading required package: lattice\n\nlibrary(e1071)\nlibrary(randomForest) \n\nrandomForest 4.7-1.1\n\n\nType rfNews() to see new features/changes/bug fixes.\n\n\n\nAttaching package: 'randomForest'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    margin\n\nlibrary(rpart)\nlibrary(naivebayes)\n\nnaivebayes 0.9.7 loaded\n\nlibrary(ROCR)\n\n# Create a random well-structured DataFrame\nset.seed(456)\nn &lt;- 200\nrandom_data &lt;- data.frame(\n  Feature1 = rnorm(n),\n  Feature2 = rnorm(n),\n  Feature3 = rnorm(n),\n  Target = factor(sample(c(\"Low\", \"High\"), n, replace = TRUE))\n)\n\n# Display the first few rows of the dataset\nhead(random_data)\n\n    Feature1     Feature2      Feature3 Target\n1 -1.3435214  0.293886215  0.7173489288   High\n2  0.6217756  1.052402224 -0.4173373241   High\n3  0.8008747 -0.006435601 -0.0315243903   High\n4 -1.3888924 -0.404523473 -0.9048116128   High\n5 -0.7143569 -0.080917045 -0.0005219186    Low\n6 -0.3240611 -1.072424293  1.3978537288    Low\n\n# Split the dataset into training and testing sets\nset.seed(123)\ntrain_index &lt;- createDataPartition(random_data$Target, p = 0.7, list = FALSE)\ntrain_data &lt;- random_data[train_index, ]\ntest_data &lt;- random_data[-train_index, ]\n\n# Support Vector Machine (SVM) model\nsvm_model &lt;- svm(Target ~ ., data = train_data)\nsvm_predictions &lt;- predict(svm_model, newdata = test_data)\n\n# Random Forest model\nrf_model &lt;- randomForest(Target ~ ., data = train_data, ntree = 100)\nrf_predictions &lt;- predict(rf_model, newdata = test_data)\n\n# Decision Tree model\ndt_model &lt;- rpart(Target ~ ., data = train_data, method = \"class\")\ndt_predictions &lt;- predict(dt_model, newdata = test_data, type = \"class\")\n\n# Naive Bayes model\nnb_model &lt;- naiveBayes(Target ~ ., data = train_data)\nnb_predictions &lt;- predict(nb_model, newdata = test_data)\n\n# Evaluate models using caret package\nsvm_metrics &lt;- confusionMatrix(svm_predictions, test_data$Target)\nrf_metrics &lt;- confusionMatrix(rf_predictions, test_data$Target)\ndt_metrics &lt;- confusionMatrix(dt_predictions, test_data$Target)\nnb_metrics &lt;- confusionMatrix(nb_predictions, test_data$Target)\n\n# Print confusion matrices\nprint(\"Confusion Matrix for SVM:\")\n\n[1] \"Confusion Matrix for SVM:\"\n\nprint(svm_metrics)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction High Low\n      High   14  19\n      Low    16  11\n                                          \n               Accuracy : 0.4167          \n                 95% CI : (0.2907, 0.5512)\n    No Information Rate : 0.5             \n    P-Value [Acc &gt; NIR] : 0.9225          \n                                          \n                  Kappa : -0.1667         \n                                          \n Mcnemar's Test P-Value : 0.7353          \n                                          \n            Sensitivity : 0.4667          \n            Specificity : 0.3667          \n         Pos Pred Value : 0.4242          \n         Neg Pred Value : 0.4074          \n             Prevalence : 0.5000          \n         Detection Rate : 0.2333          \n   Detection Prevalence : 0.5500          \n      Balanced Accuracy : 0.4167          \n                                          \n       'Positive' Class : High            \n                                          \n\nprint(\"Confusion Matrix for Random Forest:\")\n\n[1] \"Confusion Matrix for Random Forest:\"\n\nprint(rf_metrics)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction High Low\n      High   18  20\n      Low    12  10\n                                       \n               Accuracy : 0.4667       \n                 95% CI : (0.3367, 0.6)\n    No Information Rate : 0.5          \n    P-Value [Acc &gt; NIR] : 0.7405       \n                                       \n                  Kappa : -0.0667      \n                                       \n Mcnemar's Test P-Value : 0.2159       \n                                       \n            Sensitivity : 0.6000       \n            Specificity : 0.3333       \n         Pos Pred Value : 0.4737       \n         Neg Pred Value : 0.4545       \n             Prevalence : 0.5000       \n         Detection Rate : 0.3000       \n   Detection Prevalence : 0.6333       \n      Balanced Accuracy : 0.4667       \n                                       \n       'Positive' Class : High         \n                                       \n\nprint(\"Confusion Matrix for Decision Tree:\")\n\n[1] \"Confusion Matrix for Decision Tree:\"\n\nprint(dt_metrics)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction High Low\n      High   22  22\n      Low     8   8\n                                          \n               Accuracy : 0.5             \n                 95% CI : (0.3681, 0.6319)\n    No Information Rate : 0.5             \n    P-Value [Acc &gt; NIR] : 0.55129         \n                                          \n                  Kappa : 0               \n                                          \n Mcnemar's Test P-Value : 0.01762         \n                                          \n            Sensitivity : 0.7333          \n            Specificity : 0.2667          \n         Pos Pred Value : 0.5000          \n         Neg Pred Value : 0.5000          \n             Prevalence : 0.5000          \n         Detection Rate : 0.3667          \n   Detection Prevalence : 0.7333          \n      Balanced Accuracy : 0.5000          \n                                          \n       'Positive' Class : High            \n                                          \n\nprint(\"Confusion Matrix for Naive Bayes:\")\n\n[1] \"Confusion Matrix for Naive Bayes:\"\n\nprint(nb_metrics)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction High Low\n      High   10  17\n      Low    20  13\n                                          \n               Accuracy : 0.3833          \n                 95% CI : (0.2607, 0.5179)\n    No Information Rate : 0.5             \n    P-Value [Acc &gt; NIR] : 0.9741          \n                                          \n                  Kappa : -0.2333         \n                                          \n Mcnemar's Test P-Value : 0.7423          \n                                          \n            Sensitivity : 0.3333          \n            Specificity : 0.4333          \n         Pos Pred Value : 0.3704          \n         Neg Pred Value : 0.3939          \n             Prevalence : 0.5000          \n         Detection Rate : 0.1667          \n   Detection Prevalence : 0.4500          \n      Balanced Accuracy : 0.3833          \n                                          \n       'Positive' Class : High            \n                                          \n\n# Colorful EDA Visualizations\n# Example: Pair plot with color-coded Target variable\nggplot(random_data, aes(x = Feature1, y = Feature2, color = Target)) +\n  geom_point() +\n  labs(title = \"Pair Plot with Color-Coded Target Variable\")\n\n\n\n# Feature Importance Plot for Random Forest\nvarImpPlot(rf_model, main = \"Random Forest - Feature Importance\")"
  },
  {
    "objectID": "posts/final project/index.html",
    "href": "posts/final project/index.html",
    "title": "R Main explorationsS",
    "section": "",
    "text": "In this R code , we embark on an Exploratory Data Analysis (EDA) journey using a randomly generated dataset. Our exploration covers visualizing missing values, computing summary statistics, and creating a variety of plots, including histograms, scatter plots, and boxplots, to uncover insights into the distribution and relationships within variables like Age, Height, Weight, Income, Education, and Region. Additionally, we delve into correlation analysis, constructing a correlation matrix to quantify associations. The analysis culminates in fitting a linear regression model to investigate predictive relationships. This comprehensive EDA aims to reveal patterns, outliers, and potential avenues for further analysis, offering a holistic understanding of the dataset.\n\nlibrary(visdat)\n\n# Set seed for reproducibility\nset.seed(123)\n\n# Generate random data\nnum_rows <- 1000\ndata <- data.frame(\n  ID = 1:num_rows,\n  Age = sample(18:60, num_rows, replace = TRUE),\n  Height = rnorm(num_rows, mean = 170, sd = 10),\n  Weight = rnorm(num_rows, mean = 70, sd = 8),\n  Income = rnorm(num_rows, mean = 50000, sd = 10000),\n  Education = sample(c(\"High School\", \"Bachelor's\", \"Master's\", \"PhD\"), num_rows, replace = TRUE),\n  Region = sample(c(\"North\", \"South\", \"East\", \"West\"), num_rows, replace = TRUE)\n)\n\n# Introduce missing values\ndata[sample(1:num_rows, 100, replace = FALSE), \"Income\"] <- NA\n\n# Visualize missing values\nvis_dat(data)\n\n\n\n# Summary statistics\nsummary_data <- summary(data)\n\n# Plotting\npar(mfrow = c(3, 2), mar = c(4, 4, 2, 1))\n\n# Histogram of Age\nhist(data$Age, main = \"Age Distribution\", xlab = \"Age\", col = \"lightblue\", border = \"black\")\n\n# Scatter plot of Height vs Weight\nplot(data$Height, data$Weight, main = \"Height vs Weight\", xlab = \"Height\", ylab = \"Weight\", col = \"blue\")\n\n# Boxplot of Income\nboxplot(data$Income, main = \"Boxplot of Income\", col = \"lightgreen\", border = \"black\", horizontal = TRUE)\n\n# Bar plot of Education\nbarplot(table(data$Education), main = \"Education Distribution\", col = \"orange\", border = \"black\")\n\n# Pie chart of Region\npie(table(data$Region), main = \"Region Distribution\", col = rainbow(length(levels(data$Region))))\n\n# Density plot of Age\nplot(density(data$Age), main = \"Density Plot of Age\", xlab = \"Age\", col = \"purple\", lwd = 2)\n\n\n\n# Print summary statistics\ncat(\"\\nSummary Statistics:\\n\")\n\n\nSummary Statistics:\n\nprint(summary_data)\n\n       ID              Age            Height          Weight      \n Min.   :   1.0   Min.   :18.00   Min.   :143.1   Min.   : 39.40  \n 1st Qu.: 250.8   1st Qu.:28.00   1st Qu.:162.8   1st Qu.: 64.80  \n Median : 500.5   Median :40.00   Median :169.4   Median : 70.00  \n Mean   : 500.5   Mean   :39.05   Mean   :169.7   Mean   : 69.88  \n 3rd Qu.: 750.2   3rd Qu.:49.00   3rd Qu.:176.6   3rd Qu.: 75.31  \n Max.   :1000.0   Max.   :60.00   Max.   :203.0   Max.   :100.81  \n                                                                  \n     Income       Education            Region         \n Min.   :18535   Length:1000        Length:1000       \n 1st Qu.:43720   Class :character   Class :character  \n Median :49964   Mode  :character   Mode  :character  \n Mean   :50149                                        \n 3rd Qu.:56485                                        \n Max.   :78391                                        \n NA's   :100                                          \n\n# Correlation matrix\ncor_matrix <- cor(data[, c(\"Age\", \"Height\", \"Weight\", \"Income\")], use = \"complete.obs\")\ncat(\"\\nCorrelation Matrix:\\n\")\n\n\nCorrelation Matrix:\n\nprint(cor_matrix)\n\n               Age       Height       Weight      Income\nAge    1.000000000  0.003546126  0.009587395  0.05722516\nHeight 0.003546126  1.000000000 -0.023403231  0.01833394\nWeight 0.009587395 -0.023403231  1.000000000 -0.01199974\nIncome 0.057225156  0.018333936 -0.011999736  1.00000000\n\n# Regression model\nmodel <- lm(Weight ~ Age + Height + Income, data = data, na.action = na.exclude)\nsummary_model <- summary(model)\ncat(\"\\nLinear Regression Model:\\n\")\n\n\nLinear Regression Model:\n\nprint(summary_model)\n\n\nCall:\nlm(formula = Weight ~ Age + Height + Income, data = data, na.action = na.exclude)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-31.1903  -5.0391   0.1676   5.4269  31.0754 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  7.344e+01  4.871e+00  15.078   <2e-16 ***\nAge          6.939e-03  2.239e-02   0.310    0.757    \nHeight      -1.889e-02  2.718e-02  -0.695    0.487    \nIncome      -1.019e-05  2.803e-05  -0.364    0.716    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.119 on 896 degrees of freedom\n  (100 observations deleted due to missingness)\nMultiple R-squared:  0.0007887, Adjusted R-squared:  -0.002557 \nF-statistic: 0.2358 on 3 and 896 DF,  p-value: 0.8715\n\n# Save plots as PNG files\npng(\"Age_Distribution.png\")\nhist(data$Age, main = \"Age Distribution\", xlab = \"Age\", col = \"lightblue\", border = \"black\")\ndev.off()\n\npng \n  2 \n\npng(\"Height_vs_Weight.png\")\nplot(data$Height, data$Weight, main = \"Height vs Weight\", xlab = \"Height\", ylab = \"Weight\", col = \"blue\")\ndev.off()\n\npng \n  2 \n\npng(\"Income_Boxplot.png\")\nboxplot(data$Income, main = \"Boxplot of Income\", col = \"lightgreen\", border = \"black\", horizontal = TRUE)\ndev.off()\n\npng \n  2 \n\npng(\"Education_Barplot.png\")\nbarplot(table(data$Education), main = \"Education Distribution\", col = \"orange\", border = \"black\")\ndev.off()\n\npng \n  2 \n\npng(\"Region_Piechart.png\")\npie(table(data$Region), main = \"Region Distribution\", col = rainbow(length(levels(data$Region))))\ndev.off()\n\npng \n  2 \n\npng(\"Age_Density.png\")\nplot(density(data$Age), main = \"Density Plot of Age\", xlab = \"Age\", col = \"purple\", lwd = 2)\ndev.off()\n\npng \n  2 \n\n# Display saved plots\ncat(\"\\nPlots saved as PNG files: Age_Distribution.png, Height_vs_Weight.png, Income_Boxplot.png, Education_Barplot.png, Region_Piechart.png, Age_Density.png\\n\")\n\n\nPlots saved as PNG files: Age_Distribution.png, Height_vs_Weight.png, Income_Boxplot.png, Education_Barplot.png, Region_Piechart.png, Age_Density.png"
  },
  {
    "objectID": "posts/final project/index.html#intro",
    "href": "posts/final project/index.html#intro",
    "title": "R Main explorationsS",
    "section": "",
    "text": "In this R code , we embark on an Exploratory Data Analysis (EDA) journey using a randomly generated dataset. Our exploration covers visualizing missing values, computing summary statistics, and creating a variety of plots, including histograms, scatter plots, and boxplots, to uncover insights into the distribution and relationships within variables like Age, Height, Weight, Income, Education, and Region. Additionally, we delve into correlation analysis, constructing a correlation matrix to quantify associations. The analysis culminates in fitting a linear regression model to investigate predictive relationships. This comprehensive EDA aims to reveal patterns, outliers, and potential avenues for further analysis, offering a holistic understanding of the dataset.\n\nlibrary(visdat)\n\n# Set seed for reproducibility\nset.seed(123)\n\n# Generate random data\nnum_rows &lt;- 1000\ndata &lt;- data.frame(\n  ID = 1:num_rows,\n  Age = sample(18:60, num_rows, replace = TRUE),\n  Height = rnorm(num_rows, mean = 170, sd = 10),\n  Weight = rnorm(num_rows, mean = 70, sd = 8),\n  Income = rnorm(num_rows, mean = 50000, sd = 10000),\n  Education = sample(c(\"High School\", \"Bachelor's\", \"Master's\", \"PhD\"), num_rows, replace = TRUE),\n  Region = sample(c(\"North\", \"South\", \"East\", \"West\"), num_rows, replace = TRUE)\n)\n\n# Introduce missing values\ndata[sample(1:num_rows, 100, replace = FALSE), \"Income\"] &lt;- NA\n\n# Visualize missing values\nvis_dat(data)\n\n\n\n# Summary statistics\nsummary_data &lt;- summary(data)\n\n# Plotting\npar(mfrow = c(3, 2), mar = c(4, 4, 2, 1))\n\n# Histogram of Age\nhist(data$Age, main = \"Age Distribution\", xlab = \"Age\", col = \"lightblue\", border = \"black\")\n\n# Scatter plot of Height vs Weight\nplot(data$Height, data$Weight, main = \"Height vs Weight\", xlab = \"Height\", ylab = \"Weight\", col = \"blue\")\n\n# Boxplot of Income\nboxplot(data$Income, main = \"Boxplot of Income\", col = \"lightgreen\", border = \"black\", horizontal = TRUE)\n\n# Bar plot of Education\nbarplot(table(data$Education), main = \"Education Distribution\", col = \"orange\", border = \"black\")\n\n# Pie chart of Region\npie(table(data$Region), main = \"Region Distribution\", col = rainbow(length(levels(data$Region))))\n\n# Density plot of Age\nplot(density(data$Age), main = \"Density Plot of Age\", xlab = \"Age\", col = \"purple\", lwd = 2)\n\n\n\n# Print summary statistics\ncat(\"\\nSummary Statistics:\\n\")\n\n\nSummary Statistics:\n\nprint(summary_data)\n\n       ID              Age            Height          Weight      \n Min.   :   1.0   Min.   :18.00   Min.   :143.1   Min.   : 39.40  \n 1st Qu.: 250.8   1st Qu.:28.00   1st Qu.:162.8   1st Qu.: 64.80  \n Median : 500.5   Median :40.00   Median :169.4   Median : 70.00  \n Mean   : 500.5   Mean   :39.05   Mean   :169.7   Mean   : 69.88  \n 3rd Qu.: 750.2   3rd Qu.:49.00   3rd Qu.:176.6   3rd Qu.: 75.31  \n Max.   :1000.0   Max.   :60.00   Max.   :203.0   Max.   :100.81  \n                                                                  \n     Income       Education            Region         \n Min.   :18535   Length:1000        Length:1000       \n 1st Qu.:43720   Class :character   Class :character  \n Median :49964   Mode  :character   Mode  :character  \n Mean   :50149                                        \n 3rd Qu.:56485                                        \n Max.   :78391                                        \n NA's   :100                                          \n\n# Correlation matrix\ncor_matrix &lt;- cor(data[, c(\"Age\", \"Height\", \"Weight\", \"Income\")], use = \"complete.obs\")\ncat(\"\\nCorrelation Matrix:\\n\")\n\n\nCorrelation Matrix:\n\nprint(cor_matrix)\n\n               Age       Height       Weight      Income\nAge    1.000000000  0.003546126  0.009587395  0.05722516\nHeight 0.003546126  1.000000000 -0.023403231  0.01833394\nWeight 0.009587395 -0.023403231  1.000000000 -0.01199974\nIncome 0.057225156  0.018333936 -0.011999736  1.00000000\n\n# Regression model\nmodel &lt;- lm(Weight ~ Age + Height + Income, data = data, na.action = na.exclude)\nsummary_model &lt;- summary(model)\ncat(\"\\nLinear Regression Model:\\n\")\n\n\nLinear Regression Model:\n\nprint(summary_model)\n\n\nCall:\nlm(formula = Weight ~ Age + Height + Income, data = data, na.action = na.exclude)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-31.1903  -5.0391   0.1676   5.4269  31.0754 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  7.344e+01  4.871e+00  15.078   &lt;2e-16 ***\nAge          6.939e-03  2.239e-02   0.310    0.757    \nHeight      -1.889e-02  2.718e-02  -0.695    0.487    \nIncome      -1.019e-05  2.803e-05  -0.364    0.716    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.119 on 896 degrees of freedom\n  (100 observations deleted due to missingness)\nMultiple R-squared:  0.0007887, Adjusted R-squared:  -0.002557 \nF-statistic: 0.2358 on 3 and 896 DF,  p-value: 0.8715\n\n# Save plots as PNG files\npng(\"Age_Distribution.png\")\nhist(data$Age, main = \"Age Distribution\", xlab = \"Age\", col = \"lightblue\", border = \"black\")\ndev.off()\n\npng \n  2 \n\npng(\"Height_vs_Weight.png\")\nplot(data$Height, data$Weight, main = \"Height vs Weight\", xlab = \"Height\", ylab = \"Weight\", col = \"blue\")\ndev.off()\n\npng \n  2 \n\npng(\"Income_Boxplot.png\")\nboxplot(data$Income, main = \"Boxplot of Income\", col = \"lightgreen\", border = \"black\", horizontal = TRUE)\ndev.off()\n\npng \n  2 \n\npng(\"Education_Barplot.png\")\nbarplot(table(data$Education), main = \"Education Distribution\", col = \"orange\", border = \"black\")\ndev.off()\n\npng \n  2 \n\npng(\"Region_Piechart.png\")\npie(table(data$Region), main = \"Region Distribution\", col = rainbow(length(levels(data$Region))))\ndev.off()\n\npng \n  2 \n\npng(\"Age_Density.png\")\nplot(density(data$Age), main = \"Density Plot of Age\", xlab = \"Age\", col = \"purple\", lwd = 2)\ndev.off()\n\npng \n  2 \n\n# Display saved plots\ncat(\"\\nPlots saved as PNG files: Age_Distribution.png, Height_vs_Weight.png, Income_Boxplot.png, Education_Barplot.png, Region_Piechart.png, Age_Density.png\\n\")\n\n\nPlots saved as PNG files: Age_Distribution.png, Height_vs_Weight.png, Income_Boxplot.png, Education_Barplot.png, Region_Piechart.png, Age_Density.png"
  }
]