{"title":"R Main concepts blog post","markdown":{"yaml":{"title":"R Main concepts blog post","author":"Manoj Yadav Chinthalaboina ","date":"2023-12-14","categories":["news","code","analysis"],"image":"image.jpg"},"headingText":"Machine learning using real data set","containsRefs":false,"markdown":"\n\n\nIn this R script, we explore and analyze the mtcars dataset using the ggplot2 library and statistical functions. After loading the dataset, we visualize the relationship between miles per gallon and horsepower with a scatter plot. Subsequently, we subset the data to focus on cars with over 100 horsepower and perform a t-test to compare miles per gallon between high and low horsepower cars. A linear regression model is then constructed to predict miles per gallon based on horsepower, and the model is summarized for insights. Lastly, we demonstrate the use of the linear model to predict the miles per gallon for a new car with 150 horsepower, showcasing a comprehensive approach to data exploration, visualization, hypothesis testing, and predictive modeling in R.\n\n```{r}\n# Load required libraries\nlibrary(ggplot2)\nlibrary(ggthemes)\n\n# Load the built-in mtcars dataset\ndata(mtcars)\n\n# Display the structure of the dataset\nstr(mtcars)\n\n# Display the first few rows of the dataset\nhead(mtcars)\n\n# Summary statistics of the dataset\nsummary(mtcars)\n\n# Plotting a scatter plot of mpg against hp with ggplot2\nggplot(mtcars, aes(x = hp, y = mpg)) +\n  geom_point(color = \"skyblue\") +  # Add color to points\n  labs(title = \"Scatter Plot of MPG against Horsepower\",\n       x = \"Horsepower\",\n       y = \"Miles per Gallon\") +\n  theme_minimal()  # Use a minimal theme\n\n# Create a subset of the dataset for cars with more than 100 horsepower\nhigh_hp_cars <- subset(mtcars, hp > 100)\n\n# Display the first few rows of the subset\nhead(high_hp_cars)\n\n# Perform a t-test comparing miles per gallon (mpg) between high and low horsepower cars\nt_test_result <- t.test(mtcars$mpg, high_hp_cars$mpg)\nprint(t_test_result)\n\n# Linear regression model to predict mpg based on horsepower\nlinear_model <- lm(mpg ~ hp, data = mtcars)\n\n# Display the summary of the linear model\nsummary(linear_model)\n\n# Predict the mpg for a new car with 150 horsepower\nnew_car_hp <- 150\npredicted_mpg <- predict(linear_model, newdata = data.frame(hp = new_car_hp))\nprint(paste(\"Predicted MPG for a car with\", new_car_hp, \"horsepower:\", round(predicted_mpg, 2)))\n\n# Visualize the linear regression line along with the data points\nggplot(mtcars, aes(x = hp, y = mpg)) +\n  geom_point(color = \"skyblue\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +  # Add regression line\n  labs(title = \"Linear Regression: MPG against Horsepower\",\n       x = \"Horsepower\",\n       y = \"Miles per Gallon\") +\n  theme_minimal()\n\n# Residual plot to check model assumptions\nresiduals <- resid(linear_model)\nggplot(mtcars, aes(x = hp, y = residuals)) +\n  geom_point(color = \"orange\") +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"blue\") +  # Add horizontal line at y = 0\n  labs(title = \"Residual Plot\",\n       x = \"Horsepower\",\n       y = \"Residuals\") +\n  theme_minimal()\n\n```\n\nThe linear regression analysis of the mtcars dataset reveals a significant negative relationship between horsepower and miles per gallon (mpg). The model indicates that, on average, each unit increase in horsepower is associated with a decrease of 0.0682 units in mpg. The overall fit of the model is reasonable, with a low p-value and an adjusted R-squared value of 0.5892, indicating that approximately 58.92% of the variability in mpg can be explained by the linear relationship with horsepower. In practical terms, higher horsepower tends to be linked with lower fuel efficiency in the examined dataset.\n\n## SVSM RANDOM FOREST DECSISION TREE AND NAIVE BAYES MACHINE LEARNING MODEL\n\nIn this code, we are exploring the application of different machine learning models to a randomly generated dataset. The dataset consists of three features (Feature1, Feature2, and Feature3) and a binary target variable (Target) with classes \"Low\" and \"High.\" Four distinct models are trained on the training set: Support Vector Machine (SVM), Random Forest, Decision Tree, and Naive Bayes. Subsequently, each model is used to make predictions on the test set, and confusion matrices are generated to evaluate their performance. The confusion matrices provide insights into the accuracy of predictions, highlighting true positive, true negative, false positive, and false negative outcomes for each model. This analysis serves to compare and contrast the effectiveness of the diverse machine learning approaches employed in the context of the given dataset.\n\n```{r}\n# Load necessary libraries\nlibrary(ggplot2)\nlibrary(caret)\nlibrary(e1071)\nlibrary(randomForest) \nlibrary(rpart)\nlibrary(naivebayes)\nlibrary(ROCR)\n\n# Create a random well-structured DataFrame\nset.seed(456)\nn <- 200\nrandom_data <- data.frame(\n  Feature1 = rnorm(n),\n  Feature2 = rnorm(n),\n  Feature3 = rnorm(n),\n  Target = factor(sample(c(\"Low\", \"High\"), n, replace = TRUE))\n)\n\n# Display the first few rows of the dataset\nhead(random_data)\n\n# Split the dataset into training and testing sets\nset.seed(123)\ntrain_index <- createDataPartition(random_data$Target, p = 0.7, list = FALSE)\ntrain_data <- random_data[train_index, ]\ntest_data <- random_data[-train_index, ]\n\n# Support Vector Machine (SVM) model\nsvm_model <- svm(Target ~ ., data = train_data)\nsvm_predictions <- predict(svm_model, newdata = test_data)\n\n# Random Forest model\nrf_model <- randomForest(Target ~ ., data = train_data, ntree = 100)\nrf_predictions <- predict(rf_model, newdata = test_data)\n\n# Decision Tree model\ndt_model <- rpart(Target ~ ., data = train_data, method = \"class\")\ndt_predictions <- predict(dt_model, newdata = test_data, type = \"class\")\n\n# Naive Bayes model\nnb_model <- naiveBayes(Target ~ ., data = train_data)\nnb_predictions <- predict(nb_model, newdata = test_data)\n\n# Evaluate models using caret package\nsvm_metrics <- confusionMatrix(svm_predictions, test_data$Target)\nrf_metrics <- confusionMatrix(rf_predictions, test_data$Target)\ndt_metrics <- confusionMatrix(dt_predictions, test_data$Target)\nnb_metrics <- confusionMatrix(nb_predictions, test_data$Target)\n\n# Print confusion matrices\nprint(\"Confusion Matrix for SVM:\")\nprint(svm_metrics)\n\nprint(\"Confusion Matrix for Random Forest:\")\nprint(rf_metrics)\n\nprint(\"Confusion Matrix for Decision Tree:\")\nprint(dt_metrics)\n\nprint(\"Confusion Matrix for Naive Bayes:\")\nprint(nb_metrics)\n\n# Colorful EDA Visualizations\n# Example: Pair plot with color-coded Target variable\nggplot(random_data, aes(x = Feature1, y = Feature2, color = Target)) +\n  geom_point() +\n  labs(title = \"Pair Plot with Color-Coded Target Variable\")\n\n# Feature Importance Plot for Random Forest\nvarImpPlot(rf_model, main = \"Random Forest - Feature Importance\")\n\n```\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"output-file":"index.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.2.335","editor":"visual","theme":"sketchy","title-block-banner":true,"title":"R Main concepts blog post","author":"Manoj Yadav Chinthalaboina ","date":"2023-12-14","categories":["news","code","analysis"],"image":"image.jpg"},"extensions":{"book":{"multiFile":true}}}}}