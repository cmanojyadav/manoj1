{
  "hash": "6021cecf4a35512c2377736ad703eb7a",
  "result": {
    "markdown": "---\ntitle: \"R Main concepts blog post\"\nauthor: \"Manoj Yadav Chinthalaboina \"\ndate: \"2023-12-14\"\ncategories: [news, code, analysis]\nimage: \"image.jpg\"\n---\n\n\n## Machine learning using real data set\n\nIn this R script, we explore and analyze the mtcars dataset using the ggplot2 library and statistical functions. After loading the dataset, we visualize the relationship between miles per gallon and horsepower with a scatter plot. Subsequently, we subset the data to focus on cars with over 100 horsepower and perform a t-test to compare miles per gallon between high and low horsepower cars. A linear regression model is then constructed to predict miles per gallon based on horsepower, and the model is summarized for insights. Lastly, we demonstrate the use of the linear model to predict the miles per gallon for a new car with 150 horsepower, showcasing a comprehensive approach to data exploration, visualization, hypothesis testing, and predictive modeling in R.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load required libraries\nlibrary(ggplot2)\nlibrary(ggthemes)\n\n# Load the built-in mtcars dataset\ndata(mtcars)\n\n# Display the structure of the dataset\nstr(mtcars)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'data.frame':\t32 obs. of  11 variables:\n $ mpg : num  21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ...\n $ cyl : num  6 6 4 6 8 6 8 4 4 6 ...\n $ disp: num  160 160 108 258 360 ...\n $ hp  : num  110 110 93 110 175 105 245 62 95 123 ...\n $ drat: num  3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ...\n $ wt  : num  2.62 2.88 2.32 3.21 3.44 ...\n $ qsec: num  16.5 17 18.6 19.4 17 ...\n $ vs  : num  0 0 1 1 0 1 0 1 1 1 ...\n $ am  : num  1 1 1 0 0 0 0 0 0 0 ...\n $ gear: num  4 4 4 3 3 3 3 4 4 4 ...\n $ carb: num  4 4 1 1 2 1 4 2 2 4 ...\n```\n:::\n\n```{.r .cell-code}\n# Display the first few rows of the dataset\nhead(mtcars)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n```\n:::\n\n```{.r .cell-code}\n# Summary statistics of the dataset\nsummary(mtcars)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      mpg             cyl             disp             hp       \n Min.   :10.40   Min.   :4.000   Min.   : 71.1   Min.   : 52.0  \n 1st Qu.:15.43   1st Qu.:4.000   1st Qu.:120.8   1st Qu.: 96.5  \n Median :19.20   Median :6.000   Median :196.3   Median :123.0  \n Mean   :20.09   Mean   :6.188   Mean   :230.7   Mean   :146.7  \n 3rd Qu.:22.80   3rd Qu.:8.000   3rd Qu.:326.0   3rd Qu.:180.0  \n Max.   :33.90   Max.   :8.000   Max.   :472.0   Max.   :335.0  \n      drat             wt             qsec             vs        \n Min.   :2.760   Min.   :1.513   Min.   :14.50   Min.   :0.0000  \n 1st Qu.:3.080   1st Qu.:2.581   1st Qu.:16.89   1st Qu.:0.0000  \n Median :3.695   Median :3.325   Median :17.71   Median :0.0000  \n Mean   :3.597   Mean   :3.217   Mean   :17.85   Mean   :0.4375  \n 3rd Qu.:3.920   3rd Qu.:3.610   3rd Qu.:18.90   3rd Qu.:1.0000  \n Max.   :4.930   Max.   :5.424   Max.   :22.90   Max.   :1.0000  \n       am              gear            carb      \n Min.   :0.0000   Min.   :3.000   Min.   :1.000  \n 1st Qu.:0.0000   1st Qu.:3.000   1st Qu.:2.000  \n Median :0.0000   Median :4.000   Median :2.000  \n Mean   :0.4062   Mean   :3.688   Mean   :2.812  \n 3rd Qu.:1.0000   3rd Qu.:4.000   3rd Qu.:4.000  \n Max.   :1.0000   Max.   :5.000   Max.   :8.000  \n```\n:::\n\n```{.r .cell-code}\n# Plotting a scatter plot of mpg against hp with ggplot2\nggplot(mtcars, aes(x = hp, y = mpg)) +\n  geom_point(color = \"skyblue\") +  # Add color to points\n  labs(title = \"Scatter Plot of MPG against Horsepower\",\n       x = \"Horsepower\",\n       y = \"Miles per Gallon\") +\n  theme_minimal()  # Use a minimal theme\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Create a subset of the dataset for cars with more than 100 horsepower\nhigh_hp_cars <- subset(mtcars, hp > 100)\n\n# Display the first few rows of the subset\nhead(high_hp_cars)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\nDuster 360        14.3   8  360 245 3.21 3.570 15.84  0  0    3    4\n```\n:::\n\n```{.r .cell-code}\n# Perform a t-test comparing miles per gallon (mpg) between high and low horsepower cars\nt_test_result <- t.test(mtcars$mpg, high_hp_cars$mpg)\nprint(t_test_result)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tWelch Two Sample t-test\n\ndata:  mtcars$mpg and high_hp_cars$mpg\nt = 1.906, df = 52.988, p-value = 0.06209\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.1381627  5.4150648\nsample estimates:\nmean of x mean of y \n 20.09062  17.45217 \n```\n:::\n\n```{.r .cell-code}\n# Linear regression model to predict mpg based on horsepower\nlinear_model <- lm(mpg ~ hp, data = mtcars)\n\n# Display the summary of the linear model\nsummary(linear_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = mpg ~ hp, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.7121 -2.1122 -0.8854  1.5819  8.2360 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 30.09886    1.63392  18.421  < 2e-16 ***\nhp          -0.06823    0.01012  -6.742 1.79e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.863 on 30 degrees of freedom\nMultiple R-squared:  0.6024,\tAdjusted R-squared:  0.5892 \nF-statistic: 45.46 on 1 and 30 DF,  p-value: 1.788e-07\n```\n:::\n\n```{.r .cell-code}\n# Predict the mpg for a new car with 150 horsepower\nnew_car_hp <- 150\npredicted_mpg <- predict(linear_model, newdata = data.frame(hp = new_car_hp))\nprint(paste(\"Predicted MPG for a car with\", new_car_hp, \"horsepower:\", round(predicted_mpg, 2)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Predicted MPG for a car with 150 horsepower: 19.86\"\n```\n:::\n\n```{.r .cell-code}\n# Visualize the linear regression line along with the data points\nggplot(mtcars, aes(x = hp, y = mpg)) +\n  geom_point(color = \"skyblue\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +  # Add regression line\n  labs(title = \"Linear Regression: MPG against Horsepower\",\n       x = \"Horsepower\",\n       y = \"Miles per Gallon\") +\n  theme_minimal()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-1-2.png){width=672}\n:::\n\n```{.r .cell-code}\n# Residual plot to check model assumptions\nresiduals <- resid(linear_model)\nggplot(mtcars, aes(x = hp, y = residuals)) +\n  geom_point(color = \"orange\") +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"blue\") +  # Add horizontal line at y = 0\n  labs(title = \"Residual Plot\",\n       x = \"Horsepower\",\n       y = \"Residuals\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-1-3.png){width=672}\n:::\n:::\n\n\nThe linear regression analysis of the mtcars dataset reveals a significant negative relationship between horsepower and miles per gallon (mpg). The model indicates that, on average, each unit increase in horsepower is associated with a decrease of 0.0682 units in mpg. The overall fit of the model is reasonable, with a low p-value and an adjusted R-squared value of 0.5892, indicating that approximately 58.92% of the variability in mpg can be explained by the linear relationship with horsepower. In practical terms, higher horsepower tends to be linked with lower fuel efficiency in the examined dataset.\n\n## SVSM RANDOM FOREST DECSISION TREE AND NAIVE BAYES MACHINE LEARNING MODEL\n\nIn this code, we are exploring the application of different machine learning models to a randomly generated dataset. The dataset consists of three features (Feature1, Feature2, and Feature3) and a binary target variable (Target) with classes \"Low\" and \"High.\" Four distinct models are trained on the training set: Support Vector Machine (SVM), Random Forest, Decision Tree, and Naive Bayes. Subsequently, each model is used to make predictions on the test set, and confusion matrices are generated to evaluate their performance. The confusion matrices provide insights into the accuracy of predictions, highlighting true positive, true negative, false positive, and false negative outcomes for each model. This analysis serves to compare and contrast the effectiveness of the diverse machine learning approaches employed in the context of the given dataset.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load necessary libraries\nlibrary(ggplot2)\nlibrary(caret)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: lattice\n```\n:::\n\n```{.r .cell-code}\nlibrary(e1071)\nlibrary(randomForest) \n```\n\n::: {.cell-output .cell-output-stderr}\n```\nrandomForest 4.7-1.1\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nType rfNews() to see new features/changes/bug fixes.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'randomForest'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:ggplot2':\n\n    margin\n```\n:::\n\n```{.r .cell-code}\nlibrary(rpart)\nlibrary(naivebayes)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nnaivebayes 0.9.7 loaded\n```\n:::\n\n```{.r .cell-code}\nlibrary(ROCR)\n\n# Create a random well-structured DataFrame\nset.seed(456)\nn <- 200\nrandom_data <- data.frame(\n  Feature1 = rnorm(n),\n  Feature2 = rnorm(n),\n  Feature3 = rnorm(n),\n  Target = factor(sample(c(\"Low\", \"High\"), n, replace = TRUE))\n)\n\n# Display the first few rows of the dataset\nhead(random_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    Feature1     Feature2      Feature3 Target\n1 -1.3435214  0.293886215  0.7173489288   High\n2  0.6217756  1.052402224 -0.4173373241   High\n3  0.8008747 -0.006435601 -0.0315243903   High\n4 -1.3888924 -0.404523473 -0.9048116128   High\n5 -0.7143569 -0.080917045 -0.0005219186    Low\n6 -0.3240611 -1.072424293  1.3978537288    Low\n```\n:::\n\n```{.r .cell-code}\n# Split the dataset into training and testing sets\nset.seed(123)\ntrain_index <- createDataPartition(random_data$Target, p = 0.7, list = FALSE)\ntrain_data <- random_data[train_index, ]\ntest_data <- random_data[-train_index, ]\n\n# Support Vector Machine (SVM) model\nsvm_model <- svm(Target ~ ., data = train_data)\nsvm_predictions <- predict(svm_model, newdata = test_data)\n\n# Random Forest model\nrf_model <- randomForest(Target ~ ., data = train_data, ntree = 100)\nrf_predictions <- predict(rf_model, newdata = test_data)\n\n# Decision Tree model\ndt_model <- rpart(Target ~ ., data = train_data, method = \"class\")\ndt_predictions <- predict(dt_model, newdata = test_data, type = \"class\")\n\n# Naive Bayes model\nnb_model <- naiveBayes(Target ~ ., data = train_data)\nnb_predictions <- predict(nb_model, newdata = test_data)\n\n# Evaluate models using caret package\nsvm_metrics <- confusionMatrix(svm_predictions, test_data$Target)\nrf_metrics <- confusionMatrix(rf_predictions, test_data$Target)\ndt_metrics <- confusionMatrix(dt_predictions, test_data$Target)\nnb_metrics <- confusionMatrix(nb_predictions, test_data$Target)\n\n# Print confusion matrices\nprint(\"Confusion Matrix for SVM:\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Confusion Matrix for SVM:\"\n```\n:::\n\n```{.r .cell-code}\nprint(svm_metrics)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nConfusion Matrix and Statistics\n\n          Reference\nPrediction High Low\n      High   14  19\n      Low    16  11\n                                          \n               Accuracy : 0.4167          \n                 95% CI : (0.2907, 0.5512)\n    No Information Rate : 0.5             \n    P-Value [Acc > NIR] : 0.9225          \n                                          \n                  Kappa : -0.1667         \n                                          \n Mcnemar's Test P-Value : 0.7353          \n                                          \n            Sensitivity : 0.4667          \n            Specificity : 0.3667          \n         Pos Pred Value : 0.4242          \n         Neg Pred Value : 0.4074          \n             Prevalence : 0.5000          \n         Detection Rate : 0.2333          \n   Detection Prevalence : 0.5500          \n      Balanced Accuracy : 0.4167          \n                                          \n       'Positive' Class : High            \n                                          \n```\n:::\n\n```{.r .cell-code}\nprint(\"Confusion Matrix for Random Forest:\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Confusion Matrix for Random Forest:\"\n```\n:::\n\n```{.r .cell-code}\nprint(rf_metrics)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nConfusion Matrix and Statistics\n\n          Reference\nPrediction High Low\n      High   18  20\n      Low    12  10\n                                       \n               Accuracy : 0.4667       \n                 95% CI : (0.3367, 0.6)\n    No Information Rate : 0.5          \n    P-Value [Acc > NIR] : 0.7405       \n                                       \n                  Kappa : -0.0667      \n                                       \n Mcnemar's Test P-Value : 0.2159       \n                                       \n            Sensitivity : 0.6000       \n            Specificity : 0.3333       \n         Pos Pred Value : 0.4737       \n         Neg Pred Value : 0.4545       \n             Prevalence : 0.5000       \n         Detection Rate : 0.3000       \n   Detection Prevalence : 0.6333       \n      Balanced Accuracy : 0.4667       \n                                       \n       'Positive' Class : High         \n                                       \n```\n:::\n\n```{.r .cell-code}\nprint(\"Confusion Matrix for Decision Tree:\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Confusion Matrix for Decision Tree:\"\n```\n:::\n\n```{.r .cell-code}\nprint(dt_metrics)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nConfusion Matrix and Statistics\n\n          Reference\nPrediction High Low\n      High   22  22\n      Low     8   8\n                                          \n               Accuracy : 0.5             \n                 95% CI : (0.3681, 0.6319)\n    No Information Rate : 0.5             \n    P-Value [Acc > NIR] : 0.55129         \n                                          \n                  Kappa : 0               \n                                          \n Mcnemar's Test P-Value : 0.01762         \n                                          \n            Sensitivity : 0.7333          \n            Specificity : 0.2667          \n         Pos Pred Value : 0.5000          \n         Neg Pred Value : 0.5000          \n             Prevalence : 0.5000          \n         Detection Rate : 0.3667          \n   Detection Prevalence : 0.7333          \n      Balanced Accuracy : 0.5000          \n                                          \n       'Positive' Class : High            \n                                          \n```\n:::\n\n```{.r .cell-code}\nprint(\"Confusion Matrix for Naive Bayes:\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Confusion Matrix for Naive Bayes:\"\n```\n:::\n\n```{.r .cell-code}\nprint(nb_metrics)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nConfusion Matrix and Statistics\n\n          Reference\nPrediction High Low\n      High   10  17\n      Low    20  13\n                                          \n               Accuracy : 0.3833          \n                 95% CI : (0.2607, 0.5179)\n    No Information Rate : 0.5             \n    P-Value [Acc > NIR] : 0.9741          \n                                          \n                  Kappa : -0.2333         \n                                          \n Mcnemar's Test P-Value : 0.7423          \n                                          \n            Sensitivity : 0.3333          \n            Specificity : 0.4333          \n         Pos Pred Value : 0.3704          \n         Neg Pred Value : 0.3939          \n             Prevalence : 0.5000          \n         Detection Rate : 0.1667          \n   Detection Prevalence : 0.4500          \n      Balanced Accuracy : 0.3833          \n                                          \n       'Positive' Class : High            \n                                          \n```\n:::\n\n```{.r .cell-code}\n# Colorful EDA Visualizations\n# Example: Pair plot with color-coded Target variable\nggplot(random_data, aes(x = Feature1, y = Feature2, color = Target)) +\n  geom_point() +\n  labs(title = \"Pair Plot with Color-Coded Target Variable\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Feature Importance Plot for Random Forest\nvarImpPlot(rf_model, main = \"Random Forest - Feature Importance\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-2.png){width=672}\n:::\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}